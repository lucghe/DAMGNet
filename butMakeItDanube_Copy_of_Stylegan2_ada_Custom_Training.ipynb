{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "butMakeItDanube Copy of Stylegan2-ada Custom Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucghe/DAMGNet/blob/main/butMakeItDanube_Copy_of_Stylegan2_ada_Custom_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPI5E5y0pujD"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <a\n",
        "    href=\"https://youtu.be/dcb4Ckpkx2o\"\n",
        "    target=\"_blank\"\n",
        "    rel=\"noopener noreferrer\">\n",
        "        <img\n",
        "        alt=\"Night Sky Latent Walk\"\n",
        "        width=\"350\" height=\"350\"\n",
        "        src=\"https://github.com/ArthurFDLR/GANightSky/blob/main/.github/random_walk.gif?raw=true\">\n",
        "    </a>\n",
        "</p>\n",
        "\n",
        "# ðŸš€ StyleGan2-ADA for Google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktqaMJUZuOl7"
      },
      "source": [
        "1.   [Install StyleGAN2-ADA on your Google Drive](#scrollTo=5YcUMPQp6ipP)\n",
        "2.   [Train a custom model](#scrollTo=Ti11YiPAiQpb)\n",
        "3.   [Generate images from pre-trained model](#scrollTo=f0A9ZNtferpk)\n",
        "4.   [Latent space exploration](#scrollTo=5yG1UyHXXqsO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YcUMPQp6ipP"
      },
      "source": [
        "## Install StyleGAN2-ADA on your Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI_i1MwgpzOD"
      },
      "source": [
        "StyleGAN2-ADA only works with Tensorflow 1. Run the next cell before anything else to make sure weâ€™re using TF1 and not TF2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKYAU7Wub3WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a46a62b-e53b-4523-c967-85564a92ea86"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Thu Jan  6 11:51:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_1uXab3gND"
      },
      "source": [
        "Then, mount your Drive to the Colab notebook: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxYlEKI9Gis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0da8785-5926-4ddf-a55c-9fa8da8af827"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "content_path = Path('/').absolute() / 'content'\n",
        "drive_path = content_path / 'drive'\n",
        "drive.mount(str(drive_path))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epV6TDzAjox1"
      },
      "source": [
        "Finally, run this cell to install StyleGAN2-ADA on your Drive. If youâ€™ve already installed the repository, it will skip the installation process and only check for updates. If you havenâ€™t installed it, it will install all the necessary files. Beside, **in**, **out**, **datasets** and **training** folders are generated for data storage. Everything will be available on your Google Drive in the folder **StyleGAN2-ADA** even after closing this Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HX77jscX2zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2201e1-567f-4199-c303-8a62f8ed6ec8"
      },
      "source": [
        "stylegan2_repo_url  = 'https://github.com/dvschultz/stylegan2-ada' # or https://github.com/NVlabs/stylegan2-ada\n",
        "project_path        = drive_path / 'MyDrive' / 'StyleGAN2-ADA'\n",
        "stylegan2_repo_path = project_path / 'stylegan2-ada'\n",
        "\n",
        "# Create project folder if inexistant\n",
        "if not project_path.is_dir():\n",
        "    %mkdir \"{project_path}\"\n",
        "%cd \"{project_path}\"\n",
        "\n",
        "for dir in ['in', 'out', 'datasets', 'training']:\n",
        "    if not (project_path / dir).is_dir():\n",
        "        %mkdir {dir}\n",
        "if not (project_path / 'datasets' / 'source').is_dir():\n",
        "    %mkdir \"{project_path / 'datasets' / 'source'}\"\n",
        "\n",
        "# Download StyleGAN2-ada\n",
        "!git config --global user.name \"ArthurFDLR\"\n",
        "!git config --global user.email \"arthfind@gmail.com\"\n",
        "if stylegan2_repo_path.is_dir():\n",
        "    !git -C \"{stylegan2_repo_path}\" fetch origin\n",
        "    !git -C \"{stylegan2_repo_path}\" checkout origin/main -- *.py\n",
        "else:\n",
        "    print(\"Install StyleGAN2-ADA\")\n",
        "    !git clone {stylegan2_repo_url}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/StyleGAN2-ADA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti11YiPAiQpb"
      },
      "source": [
        "## Train a custom model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioqYi9NzkUfG"
      },
      "source": [
        "Once you have installed StyleGAN2-ADA on your Google Drive and set up the working directory, you can upload your training dataset images in the associated folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlV5HIEqiZvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bea5e2c-ef85-4b74-eb66-56a3000a76f0"
      },
      "source": [
        "dataset_name = 'danube'\n",
        "datasets_source_path = project_path / 'datasets' / 'source' / (dataset_name + '.zip')\n",
        "if datasets_source_path.is_dir():\n",
        "    print(\"Dataset ready for import.\")\n",
        "else:\n",
        "    print('Upload your images dataset as {}'.format(datasets_source_path))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your images dataset as /content/drive/MyDrive/StyleGAN2-ADA/datasets/source/danube.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y1-tvr5617d"
      },
      "source": [
        "Unfortunately, large datasets might exceed the Google Drive quota after a few training batches. Indeed, StyleGAN2 download datasets multiple times during training. You might have to import your dataset in the local storage session. However, large files cannot be copy/paste from Drive *(Input/Output error)*. \n",
        "\n",
        "Run this cell to download your zipped dataset from your Drive and unzip it in the local session."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_dataset_path = content_path / 'dataset'\n",
        "print(\"Importing dataset...\")\n",
        "%mkdir \"{local_dataset_path}\"\n",
        "%cp -a \"{project_path / 'datasets' / 'source' / (dataset_name + '.zip')}\" \"{local_dataset_path}\"\n",
        "print(\"Zip file succesfuly imported\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt6U1ZmLPwqb",
        "outputId": "9d26a8b7-30a4-46cc-ee55-37781fa33f30"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing dataset...\n",
            "mkdir: cannot create directory â€˜/content/datasetâ€™: File exists\n",
            "Zip file succesfuly imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQZGo4g5y7rh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f74b5469-07f6-486a-cc0e-76021f78e771"
      },
      "source": [
        "local_dataset_path = content_path / 'dataset'\n",
        "if not local_dataset_path.is_dir():\n",
        "    print(\"Importing dataset...\")\n",
        "    %mkdir \"{local_dataset_path}\"\n",
        "    %cp -a \"{project_path / 'datasets' / 'source' / (dataset_name + '.zip')}\" \"{local_dataset_path}\"\n",
        "    print(\"Zip file succesfuly imported\")\n",
        "else:\n",
        "    print('Zip file allready imported')\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(str(local_dataset_path / (dataset_name + '.zip')), 'r') as zip_ref:\n",
        "    zip_ref.extractall(str(local_dataset_path))\n",
        "print('Extraction completed')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file allready imported\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e914045b9669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dataset_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extraction completed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/danube.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(str(local_dataset_path / (dataset_name + '.zip')), 'r') as zip_ref:\n",
        "    zip_ref.extractall(str(local_dataset_path))\n",
        "print('Extraction completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_doqBu6lP_Wh",
        "outputId": "fc678968-be8d-4b59-a758-40b777981e2c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/dataset/\n",
        "%pwd\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRQYBgg2Pe3A",
        "outputId": "659ed0d8-a1e9-4019-e959-4dcf2e4e7039"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n",
            "Chemical_Status_GWB_Stamen_Watercolor_512x512_center.jpg\n",
            "Chemical_Status_GWB_Stamen_Watercolor_512x512_left.jpg\n",
            "Chemical_Status_GWB_Stamen_Watercolor_512x512_right.jpg\n",
            "Continuity_Interruptions_Ecological_Prioritisation_Stamen_Watercolor_512x512_center.jpg\n",
            "Continuity_Interruptions_Ecological_Prioritisation_Stamen_Watercolor_512x512_left.jpg\n",
            "Continuity_Interruptions_Ecological_Prioritisation_Stamen_Watercolor_512x512_right.jpg\n",
            "danube.zip\n",
            "Ecoregions_Stamen_Watercolor_512x512_center.jpg\n",
            "Ecoregions_Stamen_Watercolor_512x512_left.jpg\n",
            "Fish_Migration_Improvements_by_2021_Stamen_Watercolor_512x512_center.jpg\n",
            "Fish_Migration_Improvements_by_2021_Stamen_Watercolor_512x512_left.jpg\n",
            "Fish_Migration_Improvements_by_2021_Stamen_Watercolor_512x512_right.jpg\n",
            "Flood_Hazard_and_Flooding_Scenarios_Stamen_Watercolor_512x512_center.jpg\n",
            "Flood_Hazard_and_Flooding_Scenarios_Stamen_Watercolor_512x512_left.jpg\n",
            "Flood_Hazard_and_Flooding_Scenarios_Stamen_Watercolor_512x512_right.jpg\n",
            "Heavily_Modified_and_Artificial_SWB_Stamen_Watercolor_512x512_center.jpg\n",
            "Heavily_Modified_and_Artificial_SWB_Stamen_Watercolor_512x512_left.jpg\n",
            "Heavily_Modified_and_Artificial_SWB_Stamen_Watercolor_512x512_right.jpg\n",
            "map001_3a450a00_lft.jpg\n",
            "map001_3a450a00_rgt.jpg\n",
            "map002_0f9cfb64.jpg\n",
            "map002_3af77791_ctr.jpg\n",
            "map002_3af77791_lft.jpg\n",
            "map002_e5b38ca1.jpg\n",
            "map003_4e86efc3_top.jpg\n",
            "map003_d7b3bef0_ctr.jpg\n",
            "map003_d883b645_ctr.jpg\n",
            "map003_d8d5c47e.jpg\n",
            "map004_0617c62c.jpg\n",
            "map004_dc2a0e87_ctr.jpg\n",
            "map005_5732d95d_lft.jpg\n",
            "map005_a2bceb30.jpg\n",
            "map006_69018c07.jpg\n",
            "map007_926e7954.jpg\n",
            "map007_a39834cb.jpg\n",
            "map008_0d9209b4.jpg\n",
            "map008_b6c75496.jpg\n",
            "map008_bed21ca2_ctr.jpg\n",
            "map009_5a99dc6e.jpg\n",
            "map009_c80e9687_ctr.jpg\n",
            "map009_c80e9687_rgt.jpg\n",
            "map009_e010e4f0_lft.jpg\n",
            "map009_e010e4f0_rgt.jpg\n",
            "map010_2ec5c0d7.jpg\n",
            "map010_ac9c41c5.jpg\n",
            "map011_5ebbf124.jpg\n",
            "map011_69f7e447.jpg\n",
            "map012_3e34b66d_ctr.jpg\n",
            "map012_3e34b66d_rgt.jpg\n",
            "map012_810adede_rgt.jpg\n",
            "map013_2a4a00de.jpg\n",
            "map013_c99f62b6_ctr.jpg\n",
            "map013_c99f62b6_rgt.jpg\n",
            "map014_b945a0cd.jpg\n",
            "map014_ed17b541.jpg\n",
            "map015_61f43a76.jpg\n",
            "map015_679e02d6.jpg\n",
            "map015_c90c6f7c.jpg\n",
            "map016_cdfdff1c.jpg\n",
            "map017_05725932.jpg\n",
            "map017_18543166.jpg\n",
            "map017_7151fea6_ctr.jpg\n",
            "map018_09cf3f57_top.jpg\n",
            "map018_20cc3b2f.jpg\n",
            "map019_3613ab64.jpg\n",
            "map019_3a4914d2.jpg\n",
            "map019_a60d8768_ctr.jpg\n",
            "map020_6012f38c_lft.jpg\n",
            "map021_4e843922.jpg\n",
            "map021_e4e0e8e3.jpg\n",
            "map022_26b52e60_lft.jpg\n",
            "map022_ef6445c5_ctr.jpg\n",
            "map023_01e97018_ctr.jpg\n",
            "map023_b84afbe9_lft.jpg\n",
            "map023_d9e68b63_lft.jpg\n",
            "map024_7a5c2ad7_ctr.jpg\n",
            "map024_ab8cf548_lft.jpg\n",
            "map025_5ea3a70f.jpg\n",
            "map025_6c9f7287.jpg\n",
            "map025_d8feb89d_lft.jpg\n",
            "map026_9ee6c922.jpg\n",
            "map026_ebe35f09.jpg\n",
            "map027_6d28cf01_ctr.jpg\n",
            "map027_6d28cf01_top.jpg\n",
            "map028_1c271e5b.jpg\n",
            "map041_772456b7.jpg\n",
            "map041_801d259f.jpg\n",
            "map043_22c6fcf9_ctr.jpg\n",
            "map043_79bc652a_ctr.jpg\n",
            "map043_9e193183.jpg\n",
            "map048_7525fef7.jpg\n",
            "map049_0515f0af.jpg\n",
            "map059_97d1695a_lft.jpg\n",
            "map059_d0fc7fb0_rgt.jpg\n",
            "map060_329cf8f6_ctr.jpg\n",
            "map060_62f71d78_lft.jpg\n",
            "map060_771cc229_ctr.jpg\n",
            "map061_d1539f73_bot.jpg\n",
            "map065_9dc95e3c_top.jpg\n",
            "map065_d17d8690_ctr.jpg\n",
            "map084_f7c24749_ctr.jpg\n",
            "map086_e6cf56a3.jpg\n",
            "map087_011cd1ab_ctr.jpg\n",
            "map105_2417e278.jpg\n",
            "map106_bf0628a0_ctr.jpg\n",
            "map109_fe9101ab_ctr.jpg\n",
            "map120_a6185880_lft.jpg\n",
            "map121_09700d0b_ctr.jpg\n",
            "map121_bd81fdf4.jpg\n",
            "map126_6abc7013_lft.jpg\n",
            "map130_3319b1d9.jpg\n",
            "map132_92227d9c_bot.jpg\n",
            "map162_660851be_bot.jpg\n",
            "map250_482af812_rgt.jpg\n",
            "map272_fd5dc919_top.jpg\n",
            "map445_6aec7107_lft.jpg\n",
            "map455_a1757e3a_ctr.jpg\n",
            "map609_6a37d0b5_lft.jpg\n",
            "Nitrates_Vulnerable_Zones_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrates_Vulnerable_Zones_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrates_Vulnerable_Zones_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Baseline_Rural_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Baseline_Rural_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Baseline_Rural_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Baseline_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Baseline_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Baseline_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Rural_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Rural_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Rural_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Phosphorus_Pollution_Baseline_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Phosphorus_Pollution_Baseline_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Phosphorus_Pollution_Baseline_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Phosphorus_Pollution_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Phosphorus_Pollution_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Phosphorus_Pollution_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Baseline_Scenario_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Baseline_Scenario_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Baseline_Scenario_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Midterm_Scenario_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Midterm_Scenario_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Midterm_Scenario_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Vision_Scenario_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Vision_Scenario_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Vision_Scenario_Stamen_Watercolor_512x512_right.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQwuvcWIQBvz",
        "outputId": "2f5bbade-f62c-43cb-e931-d6d680d180dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chemical_Status_GWB_Stamen_Watercolor_512x512_center.jpg\n",
            "Chemical_Status_GWB_Stamen_Watercolor_512x512_left.jpg\n",
            "Chemical_Status_GWB_Stamen_Watercolor_512x512_right.jpg\n",
            "Continuity_Interruptions_Ecological_Prioritisation_Stamen_Watercolor_512x512_center.jpg\n",
            "Continuity_Interruptions_Ecological_Prioritisation_Stamen_Watercolor_512x512_left.jpg\n",
            "Continuity_Interruptions_Ecological_Prioritisation_Stamen_Watercolor_512x512_right.jpg\n",
            "Ecoregions_Stamen_Watercolor_512x512_center.jpg\n",
            "Ecoregions_Stamen_Watercolor_512x512_left.jpg\n",
            "Fish_Migration_Improvements_by_2021_Stamen_Watercolor_512x512_center.jpg\n",
            "Fish_Migration_Improvements_by_2021_Stamen_Watercolor_512x512_left.jpg\n",
            "Fish_Migration_Improvements_by_2021_Stamen_Watercolor_512x512_right.jpg\n",
            "Flood_Hazard_and_Flooding_Scenarios_Stamen_Watercolor_512x512_center.jpg\n",
            "Flood_Hazard_and_Flooding_Scenarios_Stamen_Watercolor_512x512_left.jpg\n",
            "Flood_Hazard_and_Flooding_Scenarios_Stamen_Watercolor_512x512_right.jpg\n",
            "Heavily_Modified_and_Artificial_SWB_Stamen_Watercolor_512x512_center.jpg\n",
            "Heavily_Modified_and_Artificial_SWB_Stamen_Watercolor_512x512_left.jpg\n",
            "Heavily_Modified_and_Artificial_SWB_Stamen_Watercolor_512x512_right.jpg\n",
            "map001_3a450a00_lft.jpg\n",
            "map001_3a450a00_rgt.jpg\n",
            "map002_0f9cfb64.jpg\n",
            "map002_3af77791_ctr.jpg\n",
            "map002_3af77791_lft.jpg\n",
            "map002_e5b38ca1.jpg\n",
            "map003_4e86efc3_top.jpg\n",
            "map003_d7b3bef0_ctr.jpg\n",
            "map003_d883b645_ctr.jpg\n",
            "map003_d8d5c47e.jpg\n",
            "map004_0617c62c.jpg\n",
            "map004_dc2a0e87_ctr.jpg\n",
            "map005_5732d95d_lft.jpg\n",
            "map005_a2bceb30.jpg\n",
            "map006_69018c07.jpg\n",
            "map007_926e7954.jpg\n",
            "map007_a39834cb.jpg\n",
            "map008_0d9209b4.jpg\n",
            "map008_b6c75496.jpg\n",
            "map008_bed21ca2_ctr.jpg\n",
            "map009_5a99dc6e.jpg\n",
            "map009_c80e9687_ctr.jpg\n",
            "map009_c80e9687_rgt.jpg\n",
            "map009_e010e4f0_lft.jpg\n",
            "map009_e010e4f0_rgt.jpg\n",
            "map010_2ec5c0d7.jpg\n",
            "map010_ac9c41c5.jpg\n",
            "map011_5ebbf124.jpg\n",
            "map011_69f7e447.jpg\n",
            "map012_3e34b66d_ctr.jpg\n",
            "map012_3e34b66d_rgt.jpg\n",
            "map012_810adede_rgt.jpg\n",
            "map013_2a4a00de.jpg\n",
            "map013_c99f62b6_ctr.jpg\n",
            "map013_c99f62b6_rgt.jpg\n",
            "map014_b945a0cd.jpg\n",
            "map014_ed17b541.jpg\n",
            "map015_61f43a76.jpg\n",
            "map015_679e02d6.jpg\n",
            "map015_c90c6f7c.jpg\n",
            "map016_cdfdff1c.jpg\n",
            "map017_05725932.jpg\n",
            "map017_18543166.jpg\n",
            "map017_7151fea6_ctr.jpg\n",
            "map018_09cf3f57_top.jpg\n",
            "map018_20cc3b2f.jpg\n",
            "map019_3613ab64.jpg\n",
            "map019_3a4914d2.jpg\n",
            "map019_a60d8768_ctr.jpg\n",
            "map020_6012f38c_lft.jpg\n",
            "map021_4e843922.jpg\n",
            "map021_e4e0e8e3.jpg\n",
            "map022_26b52e60_lft.jpg\n",
            "map022_ef6445c5_ctr.jpg\n",
            "map023_01e97018_ctr.jpg\n",
            "map023_b84afbe9_lft.jpg\n",
            "map023_d9e68b63_lft.jpg\n",
            "map024_7a5c2ad7_ctr.jpg\n",
            "map024_ab8cf548_lft.jpg\n",
            "map025_5ea3a70f.jpg\n",
            "map025_6c9f7287.jpg\n",
            "map025_d8feb89d_lft.jpg\n",
            "map026_9ee6c922.jpg\n",
            "map026_ebe35f09.jpg\n",
            "map027_6d28cf01_ctr.jpg\n",
            "map027_6d28cf01_top.jpg\n",
            "map028_1c271e5b.jpg\n",
            "map041_772456b7.jpg\n",
            "map041_801d259f.jpg\n",
            "map043_22c6fcf9_ctr.jpg\n",
            "map043_79bc652a_ctr.jpg\n",
            "map043_9e193183.jpg\n",
            "map048_7525fef7.jpg\n",
            "map049_0515f0af.jpg\n",
            "map059_97d1695a_lft.jpg\n",
            "map059_d0fc7fb0_rgt.jpg\n",
            "map060_329cf8f6_ctr.jpg\n",
            "map060_62f71d78_lft.jpg\n",
            "map060_771cc229_ctr.jpg\n",
            "map061_d1539f73_bot.jpg\n",
            "map065_9dc95e3c_top.jpg\n",
            "map065_d17d8690_ctr.jpg\n",
            "map084_f7c24749_ctr.jpg\n",
            "map086_e6cf56a3.jpg\n",
            "map087_011cd1ab_ctr.jpg\n",
            "map105_2417e278.jpg\n",
            "map106_bf0628a0_ctr.jpg\n",
            "map109_fe9101ab_ctr.jpg\n",
            "map120_a6185880_lft.jpg\n",
            "map121_09700d0b_ctr.jpg\n",
            "map121_bd81fdf4.jpg\n",
            "map126_6abc7013_lft.jpg\n",
            "map130_3319b1d9.jpg\n",
            "map132_92227d9c_bot.jpg\n",
            "map162_660851be_bot.jpg\n",
            "map250_482af812_rgt.jpg\n",
            "map272_fd5dc919_top.jpg\n",
            "map445_6aec7107_lft.jpg\n",
            "map455_a1757e3a_ctr.jpg\n",
            "map609_6a37d0b5_lft.jpg\n",
            "Nitrates_Vulnerable_Zones_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrates_Vulnerable_Zones_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrates_Vulnerable_Zones_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Baseline_Rural_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Baseline_Rural_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Baseline_Rural_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Baseline_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Baseline_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Baseline_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Rural_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Rural_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Rural_Stamen_Watercolor_512x512_right.jpg\n",
            "Nitrogen_Pollution_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Nitrogen_Pollution_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Nitrogen_Pollution_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Phosphorus_Pollution_Baseline_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Phosphorus_Pollution_Baseline_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Phosphorus_Pollution_Baseline_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Phosphorus_Pollution_Total_Stamen_Watercolor_512x512_center.jpg\n",
            "Phosphorus_Pollution_Total_Stamen_Watercolor_512x512_left.jpg\n",
            "Phosphorus_Pollution_Total_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Baseline_Scenario_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Baseline_Scenario_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Baseline_Scenario_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Midterm_Scenario_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Midterm_Scenario_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Midterm_Scenario_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Stamen_Watercolor_512x512_right.jpg\n",
            "Urban_Wastewater_Treatment_Vision_Scenario_Stamen_Watercolor_512x512_center.jpg\n",
            "Urban_Wastewater_Treatment_Vision_Scenario_Stamen_Watercolor_512x512_left.jpg\n",
            "Urban_Wastewater_Treatment_Vision_Scenario_Stamen_Watercolor_512x512_right.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS9tDvt61VG"
      },
      "source": [
        "### Convert dataset to .tfrecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q58MJbckLUc"
      },
      "source": [
        "Next, we need to convert our image dataset to a format that StyleGAN2-ADA can read:`.tfrecords`.\n",
        "\n",
        "This can take a while."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd tfr/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1gREyITQUjM",
        "outputId": "7d6e0797-af29-446a-e203-9fbfb82934df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset/tfr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir trf/images"
      ],
      "metadata": {
        "id": "gT7PC5vpQZtX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjH8kBDo3kFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced5b0e6-e202-46a6-ce33-f0cd686a98ef"
      },
      "source": [
        "local_images_path = local_dataset_path / 'images'\n",
        "local_dataset_path /= 'tfr'\n",
        "\n",
        "if (local_dataset_path).is_dir():\n",
        "    print('\\N{Heavy Exclamation Mark Symbol} Dataset already created \\N{Heavy Exclamation Mark Symbol}')\n",
        "    print('Delete current dataset folder ({}) to regenerate tfrecords.'.format(local_dataset_path))\n",
        "else:\n",
        "    %mkdir \"{local_dataset_path}\"\n",
        "    !python \"{stylegan2_repo_path / 'dataset_tool.py'}\" create_from_images \\\n",
        "        \"{local_dataset_path}\" \"{local_images_path}\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜/content/dataset/tfr/tfrâ€™: No such file or directory\n",
            "Loading images from \"/content/dataset/tfr/images\"\n",
            "Error: No input images found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"{stylegan2_repo_path / 'dataset_tool.py'}\" create_from_images \"/content/dataset/trf/images\" \"/content/dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iJhmnyNQpZv",
        "outputId": "0e28d6c8-a3ae-4765-d57e-50880b77b641"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from \"/content/dataset\"\n",
            "Creating dataset \"/content/dataset/trf/images\"\n",
            "0 / 151\r/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 101 images.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dataset_tool.py\", line 1249, in <module>\n",
            "    execute_cmdline(sys.argv)\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dataset_tool.py\", line 1244, in execute_cmdline\n",
            "    func(**vars(args))\n",
            "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dataset_tool.py\", line 710, in create_from_images\n",
            "    img = np.asarray(PIL.Image.open(image_filenames[order[idx]]))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2843, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "IsADirectoryError: [Errno 21] Is a directory: '/content/dataset/trf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9E6fiNWSYY8x",
        "outputId": "319549d5-cc00-41ac-94e8-c520eeadf4a2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/dataset/trf/images'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DvTupHzP2s_"
      },
      "source": [
        "There are numerous arguments to tune the training of your model. To obtain nice results, you will certainly have to experiment. Here are the most popular parameters:\n",
        "\n",
        "\n",
        "*   *mirror:* Should the images be mirrored vertically?\n",
        "*   *mirrory:* Should the images be mirrored horizontally?\n",
        "*   *snap:* How often should the model generate image samples and a network pickle (.pkl file)?\n",
        "*   *resume:* Network pickle to resume training from?\n",
        "\n",
        "To see all the options, run the following ```help``` cell.\n",
        "\n",
        "Please note that Google Colab Pro gives access to V100 GPUs, which drastically decreases (~3x) processing time over P100 GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxu7CA0Qb1Yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816ceaa8-4808-40a7-b00f-f58339fa82f4"
      },
      "source": [
        "!python \"{stylegan2_repo_path / 'train.py'}\" --help"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
            "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
            "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
            "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
            "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
            "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
            "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
            "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
            "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
            "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
            "\n",
            "Train a GAN using the techniques described in the paper\n",
            "\"Training Generative Adversarial Networks with Limited Data\".\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "general options:\n",
            "  --outdir DIR          Where to save the results (required)\n",
            "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
            "  --snap INT            Snapshot interval (default: 50 ticks)\n",
            "  --seed INT            Random seed (default: 1000)\n",
            "  -n, --dry-run         Print training options and exit\n",
            "\n",
            "training dataset:\n",
            "  --data PATH           Training dataset path (required)\n",
            "  --res INT             Dataset resolution (default: highest available)\n",
            "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
            "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
            "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
            "                        create_from_images_raw (default: False)\n",
            "\n",
            "metrics:\n",
            "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
            "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
            "\n",
            "base config:\n",
            "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
            "                        Base config (default: auto)\n",
            "  --lrate FLOAT         Override learning rate\n",
            "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
            "                        for discriminator) (default: false)\n",
            "  --gamma FLOAT         Override R1 gamma\n",
            "  --nkimg INT           Override starting count\n",
            "  --kimg INT            Override training duration\n",
            "  --topk FLOAT          utilize top-k training\n",
            "\n",
            "discriminator augmentation:\n",
            "  --aug {noaug,ada,fixed,adarv}\n",
            "                        Augmentation mode (default: ada)\n",
            "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
            "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
            "  --initstrength INITSTRENGTH\n",
            "                        Override ADA strength at start\n",
            "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
            "                        Augmentation pipeline (default: bgc)\n",
            "\n",
            "comparison methods:\n",
            "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
            "                        Comparison method (default: nocmethod)\n",
            "  --dcap FLOAT          Multiplier for discriminator capacity\n",
            "\n",
            "transfer learning:\n",
            "  --resume RESUME       Resume from network pickle (default: noresume)\n",
            "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
            "\n",
            "examples:\n",
            "\n",
            "  # Train custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
            "      --cfg=cifar\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
            "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
            "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
            "\n",
            "available base configs (--cfg):\n",
            "  auto           Automatically select reasonable defaults based on resolution\n",
            "                 and GPU count. Good starting point for new datasets.\n",
            "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
            "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
            "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
            "\n",
            "transfer learning source networks (--resume):\n",
            "  ffhq256        FFHQ trained at 256x256 resolution.\n",
            "  ffhq512        FFHQ trained at 512x512 resolution.\n",
            "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
            "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
            "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
            "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
            "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
            "  metfaces512    MetFaces trained at 512x512 resolution.\n",
            "  <path or URL>  Custom network pickle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOftFoyiDU3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3fbf97-16f4-44b0-82c4-47e93d780bc4"
      },
      "source": [
        "\n",
        "\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 2\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = False\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "augs = 'color'\n",
        "\n",
        "resume_from = 'brecahad512'\n",
        "\n",
        "!python \"{stylegan2_repo_path / 'train.py'}\" --outdir=\"/content/drive/MyDrive/StyleGAN2-ADA/training/danube\" \\\n",
        "    --data=\"/content/dataset/trf/images\" \\\n",
        "    --snap={snapshot_count} --augpipe={augs} \\\n",
        "    --mirror={mirrored} --mirrory={mirroredY} \\\n",
        "    --metrics={metric_list} #--dry-run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x55b3f41d6000 @  0x7f986c230001 0x7f98694161af 0x7f986946cc23 0x7f986946da87 0x7f986950f823 0x55b3ec2bb46c 0x55b3ec2bb240 0x55b3ec32f627 0x55b3ec329ced 0x55b3ec2bd48c 0x55b3ec2fe159 0x55b3ec2fb0a4 0x55b3ec2bbd49 0x55b3ec32f94f 0x55b3ec3299ee 0x55b3ec1fbe2b 0x55b3ec32bfe4 0x55b3ec3299ee 0x55b3ec1fbe2b 0x55b3ec32bfe4 0x55b3ec329ced 0x55b3ec1fbe2b 0x55b3ec32bfe4 0x55b3ec2bcafa 0x55b3ec32a915 0x55b3ec3299ee 0x55b3ec3296f3 0x55b3ec3f34c2 0x55b3ec3f383d 0x55b3ec3f36e6 0x55b3ec3cb163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b4f41d6000 @  0x7f986c22e1e7 0x7f98694160ce 0x7f986946ccf5 0x7f986946cf4f 0x7f986950f673 0x55b3ec2bb46c 0x55b3ec2bb240 0x55b3ec32f627 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec2bcafa 0x55b3ec32a915 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32ed00 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec329ced 0x55b3ec2bd48c 0x55b3ec2fe159 0x55b3ec2fb0a4 0x55b3ec2bbd49 0x55b3ec32f94f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b5f4d02000 @  0x7f986c22e1e7 0x7f98694160ce 0x7f986946ccf5 0x7f986946cf4f 0x7f982ca82235 0x7f982c405792 0x7f982c405d42 0x7f982c3beaee 0x55b3ec2bb437 0x55b3ec2bb240 0x55b3ec32f0f3 0x55b3ec2bcafa 0x55b3ec32ac0d 0x55b3ec329ced 0x55b3ec1fbeb0 0x55b3ec32bfe4 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32ac0d 0x55b3ec329ced 0x55b3ec2bcbda 0x55b3ec32ac0d 0x55b3ec2bcafa 0x55b3ec32ac0d 0x55b3ec3299ee 0x55b3ec2bd271 0x55b3ec2bd698 0x55b3ec32bfe4 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32a915\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 2,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.0025\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.0025\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 6.5536\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"brightness\": 1,\n",
            "      \"contrast\": 1,\n",
            "      \"lumaflip\": 1,\n",
            "      \"hue\": 1,\n",
            "      \"saturation\": 1\n",
            "    }\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 2,\n",
            "  \"network_snapshot_ticks\": 2,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"/content/dataset/trf/images\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 512,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"/content/dataset/trf/images\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 512,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"minibatch_size\": 8,\n",
            "  \"minibatch_gpu\": 8,\n",
            "  \"G_smoothing_kimg\": 2.5,\n",
            "  \"G_smoothing_rampup\": 0.05,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/StyleGAN2-ADA/training/danube/00003-images-mirror-auto1-color\"\n",
            "}\n",
            "\n",
            "Output directory:  /content/drive/MyDrive/StyleGAN2-ADA/training/danube/00003-images-mirror-auto1-color\n",
            "Training data:     /content/dataset/trf/images\n",
            "Training length:   25000 kimg\n",
            "Resolution:        512\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b6f4d02000 @  0x7f986c230001 0x7f98694161af 0x7f986946cc23 0x7f986946da87 0x7f986950f823 0x55b3ec2bb46c 0x55b3ec2bb240 0x55b3ec32f627 0x55b3ec329ced 0x55b3ec2bd48c 0x55b3ec2fe159 0x55b3ec2fb0a4 0x55b3ec2bbd49 0x55b3ec32f94f 0x55b3ec3299ee 0x55b3ec1fbe2b 0x55b3ec32bfe4 0x55b3ec3299ee 0x55b3ec1fbe2b 0x55b3ec32bfe4 0x55b3ec329ced 0x55b3ec1fbe2b 0x55b3ec32bfe4 0x55b3ec2bcafa 0x55b3ec32a915 0x55b3ec3299ee 0x55b3ec3296f3 0x55b3ec3f34c2 0x55b3ec3f383d 0x55b3ec3f36e6 0x55b3ec3cb163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b7f5502000 @  0x7f986c22e1e7 0x7f98694160ce 0x7f986946ccf5 0x7f986946cf4f 0x7f986950f673 0x55b3ec2bb46c 0x55b3ec2bb240 0x55b3ec32f627 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec2bcafa 0x55b3ec32a915 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32ed00 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32b737 0x55b3ec329ced 0x55b3ec2bd48c 0x55b3ec2fe159 0x55b3ec2fb0a4 0x55b3ec2bbd49 0x55b3ec32f94f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55b7f5502000 @  0x7f986c22e1e7 0x7f98694160ce 0x7f986946ccf5 0x7f986946cf4f 0x7f982ca82235 0x7f982c405792 0x7f982c405d42 0x7f982c3beaee 0x55b3ec2bb437 0x55b3ec2bb240 0x55b3ec32f0f3 0x55b3ec2bcafa 0x55b3ec32ac0d 0x55b3ec329ced 0x55b3ec1fbeb0 0x55b3ec32bfe4 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32ac0d 0x55b3ec329ced 0x55b3ec2bcbda 0x55b3ec32ac0d 0x55b3ec2bcafa 0x55b3ec32ac0d 0x55b3ec3299ee 0x55b3ec2bd271 0x55b3ec2bd698 0x55b3ec32bfe4 0x55b3ec3299ee 0x55b3ec2bcbda 0x55b3ec32a915\n",
            "Image shape: [3, 512, 512]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "\n",
            "G                             Params    OutputShape         WeightShape     \n",
            "---                           ---       ---                 ---             \n",
            "latents_in                    -         (?, 512)            -               \n",
            "labels_in                     -         (?, 0)              -               \n",
            "epochs                        1         ()                  ()              \n",
            "epochs_1                      1         ()                  ()              \n",
            "G_mapping/Normalize           -         (?, 512)            -               \n",
            "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Broadcast           -         (?, 16, 512)        -               \n",
            "dlatent_avg                   -         (512,)              -               \n",
            "Truncation/Lerp               -         (?, 16, 512)        -               \n",
            "G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n",
            "G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n",
            "G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n",
            "G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up    2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1       2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n",
            "G_synthesis/64x64/ToRGB       264195    (?, 3, 64, 64)      (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up  1442561   (?, 256, 128, 128)  (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1     721409    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n",
            "G_synthesis/128x128/ToRGB     132099    (?, 3, 128, 128)    (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up  426369    (?, 128, 256, 256)  (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1     213249    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample  -         (?, 3, 256, 256)    -               \n",
            "G_synthesis/256x256/ToRGB     66051     (?, 3, 256, 256)    (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up  139457    (?, 64, 512, 512)   (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1     69761     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample  -         (?, 3, 512, 512)    -               \n",
            "G_synthesis/512x512/ToRGB     33027     (?, 3, 512, 512)    (1, 1, 64, 3)   \n",
            "---                           ---       ---                 ---             \n",
            "Total                         28700649                                      \n",
            "\n",
            "\n",
            "D                    Params    OutputShape         WeightShape     \n",
            "---                  ---       ---                 ---             \n",
            "images_in            -         (?, 3, 512, 512)    -               \n",
            "labels_in            -         (?, 0)              -               \n",
            "512x512/FromRGB      256       (?, 64, 512, 512)   (1, 1, 3, 64)   \n",
            "512x512/Conv0        36928     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
            "512x512/Conv1_down   73856     (?, 128, 256, 256)  (3, 3, 64, 128) \n",
            "512x512/Skip         8192      (?, 128, 256, 256)  (1, 1, 64, 128) \n",
            "256x256/Conv0        147584    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
            "256x256/Conv1_down   295168    (?, 256, 128, 128)  (3, 3, 128, 256)\n",
            "256x256/Skip         32768     (?, 256, 128, 128)  (1, 1, 128, 256)\n",
            "128x128/Conv0        590080    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "128x128/Conv1_down   1180160   (?, 512, 64, 64)    (3, 3, 256, 512)\n",
            "128x128/Skip         131072    (?, 512, 64, 64)    (1, 1, 256, 512)\n",
            "64x64/Conv0          2359808   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "64x64/Conv1_down     2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "64x64/Skip           262144    (?, 512, 32, 32)    (1, 1, 512, 512)\n",
            "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n",
            "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n",
            "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
            "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
            "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
            "Output               513       (?, 1)              (512, 1)        \n",
            "---                  ---       ---                 ---             \n",
            "Total                28982849                                      \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 2m 28s       sec/tick 62.8    sec/kimg 1961.85 maintenance 85.4   gpumem 9.9   augment 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0A9ZNtferpk"
      },
      "source": [
        "## Generate images from pre-trained model\n",
        "\n",
        "You can finally generate images using a pre-trained network once everything is set-up. You can naturally use [your own model once it is trained](#scrollTo=Ti11YiPAiQpb&uniqifier=1) or use the ones NVLab published on [their website](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/).\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img\n",
        "    alt=\"Night Sky Latent Walk\"\n",
        "    width=\"450\" height=\"300\"\n",
        "    src=\"https://github.com/ArthurFDLR/GANightSky/blob/main/.github/Random_Generation.png?raw=true\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlrb9QzgaGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0956799c-dd1b-420b-cbb7-1c97e932e4e4"
      },
      "source": [
        "%pip install opensimplex\n",
        "!python \"{stylegan2_repo_path / 'generate.py'}\" generate-images --help "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opensimplex in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from opensimplex) (1.21.5)\n",
            "usage: generate.py generate-images [-h] --network NETWORK_PKL --seeds SEEDS\n",
            "                                   [--trunc TRUNCATION_PSI]\n",
            "                                   [--class CLASS_IDX] [--create-grid]\n",
            "                                   [--outdir DIR] [--save_vector] [--fixnoise]\n",
            "                                   [--jpg_quality JPG_QUALITY]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --network NETWORK_PKL\n",
            "                        Network pickle filename\n",
            "  --seeds SEEDS         List of random seeds\n",
            "  --trunc TRUNCATION_PSI\n",
            "                        Truncation psi (default: 0.5)\n",
            "  --class CLASS_IDX     Class label (default: unconditional)\n",
            "  --create-grid         Add flag to save the generated images in a grid\n",
            "  --outdir DIR          Root directory for run results (default: out)\n",
            "  --save_vector         also save vector in .npy format\n",
            "  --fixnoise            generate images using fixed noise (more accurate for\n",
            "                        interpolations)\n",
            "  --jpg_quality JPG_QUALITY\n",
            "                        Quality compression for JPG exports (1 to 95), keep\n",
            "                        default value to export as PNG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQqYjeRsfYD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f330c4a9-9931-4d10-d354-7c471cac28bc"
      },
      "source": [
        "from numpy import random\n",
        "seed_init = random.randint(10000)\n",
        "nbr_images = 6\n",
        "\n",
        "#generation_from = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl'\n",
        "generation_from = '/content/drive/MyDrive/StyleGAN2-ADA/training/danube/00000-images-mirror-auto1-bgc-resumeffhq1024/network-snapshot-000000.pkl'\n",
        "\n",
        "!python \"{stylegan2_repo_path / 'generate.py'}\" generate-images \\\n",
        "    --outdir=\"{project_path / 'out'}\" --trunc=0.7 \\\n",
        "    --seeds={seed_init}-{seed_init+nbr_images-1} --create-grid \\\n",
        "    --network={generation_from}"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2-ADA/training/danube/00000-images-mirror-auto1-bgc-resumeffhq1024/network-snapshot-000000.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Generating image for seed 5150 (0/6) ...\n",
            "Generating image for seed 5151 (1/6) ...\n",
            "Generating image for seed 5152 (2/6) ...\n",
            "Generating image for seed 5153 (3/6) ...\n",
            "Generating image for seed 5154 (4/6) ...\n",
            "Generating image for seed 5155 (5/6) ...\n",
            "Generating image grid...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yG1UyHXXqsO"
      },
      "source": [
        "## Latent space exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYLhEFGMtJIw"
      },
      "source": [
        "It is also possible to explore the latent space associated with our model and [generate videos like this one](https://youtu.be/dcb4Ckpkx2o).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veceGR6QYA93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94247d9-a588-4795-c0c5-14f61e023001"
      },
      "source": [
        "%pip install opensimplex\n",
        "!python \"{stylegan2_repo_path / 'generate.py'}\" generate-latent-walk --help "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opensimplex in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from opensimplex) (1.21.5)\n",
            "usage: generate.py generate-latent-walk [-h] --network NETWORK_PKL\n",
            "                                        [--trunc TRUNCATION_PSI]\n",
            "                                        [--walk-type WALK_TYPE]\n",
            "                                        [--frames FRAMES] [--fps FRAMERATE]\n",
            "                                        [--seeds SEEDS] [--npys NPYS]\n",
            "                                        [--save_vector] [--diameter DIAMETER]\n",
            "                                        [--start_seed START_SEED]\n",
            "                                        [--outdir DIR]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --network NETWORK_PKL\n",
            "                        Network pickle filename\n",
            "  --trunc TRUNCATION_PSI\n",
            "                        Truncation psi (default: 0.5)\n",
            "  --walk-type WALK_TYPE\n",
            "                        Type of walk (default: line)\n",
            "  --frames FRAMES       Frame count (default: 240\n",
            "  --fps FRAMERATE       Starting value\n",
            "  --seeds SEEDS         List of random seeds\n",
            "  --npys NPYS           List of .npy files\n",
            "  --save_vector         also save vector in .npy format\n",
            "  --diameter DIAMETER   diameter of noise loop\n",
            "  --start_seed START_SEED\n",
            "                        random seed to start noise loop from\n",
            "  --outdir DIR          Root directory for run results (default: out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjsN05ksZYZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a617f024-5220-4a10-a728-06e2d470cb44"
      },
      "source": [
        "from numpy import random\n",
        "walk_types = ['line', 'sphere', 'circularloop']\n",
        "latent_walk_path = project_path / 'out' / 'latent_walk'\n",
        "if not latent_walk_path.is_dir():\n",
        "    %mkdir \"{latent_walk_path}\"\n",
        "\n",
        "#explored_network = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl'\n",
        "explored_network = '/content/drive/MyDrive/StyleGAN2-ADA/training/danube/00000-images-mirror-auto1-bgc-resumeffhq1024/network-snapshot-000000.pkl'\n",
        "\n",
        "seeds = [random.randint(10000) for i in range(10)]\n",
        "print(','.join(map(str, seeds)))\n",
        "print(\"Base seeds:\", seeds)\n",
        "!python \"{stylegan2_repo_path / 'generate.py'}\" generate-latent-walk --network=\"{explored_network}\" \\\n",
        "    --outdir=\"{latent_walk_path}\" --trunc=0.7 --walk-type=\"{walk_types[2]}\" \\\n",
        "    --seeds={','.join(map(str, seeds))} --frames {len(seeds)*20}"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5998,7229,2139,9498,305,7436,2043,3234,5551,4316\n",
            "Base seeds: [5998, 7229, 2139, 9498, 305, 7436, 2043, 3234, 5551, 4316]\n",
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2-ADA/training/danube/00000-images-mirror-auto1-bgc-resumeffhq1024/network-snapshot-000000.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Generating image for step 0/200 ...\n",
            "Generating image for step 1/200 ...\n",
            "Generating image for step 2/200 ...\n",
            "Generating image for step 3/200 ...\n",
            "Generating image for step 4/200 ...\n",
            "Generating image for step 5/200 ...\n",
            "Generating image for step 6/200 ...\n",
            "Generating image for step 7/200 ...\n",
            "Generating image for step 8/200 ...\n",
            "Generating image for step 9/200 ...\n",
            "Generating image for step 10/200 ...\n",
            "Generating image for step 11/200 ...\n",
            "Generating image for step 12/200 ...\n",
            "Generating image for step 13/200 ...\n",
            "Generating image for step 14/200 ...\n",
            "Generating image for step 15/200 ...\n",
            "Generating image for step 16/200 ...\n",
            "Generating image for step 17/200 ...\n",
            "Generating image for step 18/200 ...\n",
            "Generating image for step 19/200 ...\n",
            "Generating image for step 20/200 ...\n",
            "Generating image for step 21/200 ...\n",
            "Generating image for step 22/200 ...\n",
            "Generating image for step 23/200 ...\n",
            "Generating image for step 24/200 ...\n",
            "Generating image for step 25/200 ...\n",
            "Generating image for step 26/200 ...\n",
            "Generating image for step 27/200 ...\n",
            "Generating image for step 28/200 ...\n",
            "Generating image for step 29/200 ...\n",
            "Generating image for step 30/200 ...\n",
            "Generating image for step 31/200 ...\n",
            "Generating image for step 32/200 ...\n",
            "Generating image for step 33/200 ...\n",
            "Generating image for step 34/200 ...\n",
            "Generating image for step 35/200 ...\n",
            "Generating image for step 36/200 ...\n",
            "Generating image for step 37/200 ...\n",
            "Generating image for step 38/200 ...\n",
            "Generating image for step 39/200 ...\n",
            "Generating image for step 40/200 ...\n",
            "Generating image for step 41/200 ...\n",
            "Generating image for step 42/200 ...\n",
            "Generating image for step 43/200 ...\n",
            "Generating image for step 44/200 ...\n",
            "Generating image for step 45/200 ...\n",
            "Generating image for step 46/200 ...\n",
            "Generating image for step 47/200 ...\n",
            "Generating image for step 48/200 ...\n",
            "Generating image for step 49/200 ...\n",
            "Generating image for step 50/200 ...\n",
            "Generating image for step 51/200 ...\n",
            "Generating image for step 52/200 ...\n",
            "Generating image for step 53/200 ...\n",
            "Generating image for step 54/200 ...\n",
            "Generating image for step 55/200 ...\n",
            "Generating image for step 56/200 ...\n",
            "Generating image for step 57/200 ...\n",
            "Generating image for step 58/200 ...\n",
            "Generating image for step 59/200 ...\n",
            "Generating image for step 60/200 ...\n",
            "Generating image for step 61/200 ...\n",
            "Generating image for step 62/200 ...\n",
            "Generating image for step 63/200 ...\n",
            "Generating image for step 64/200 ...\n",
            "Generating image for step 65/200 ...\n",
            "Generating image for step 66/200 ...\n",
            "Generating image for step 67/200 ...\n",
            "Generating image for step 68/200 ...\n",
            "Generating image for step 69/200 ...\n",
            "Generating image for step 70/200 ...\n",
            "Generating image for step 71/200 ...\n",
            "Generating image for step 72/200 ...\n",
            "Generating image for step 73/200 ...\n",
            "Generating image for step 74/200 ...\n",
            "Generating image for step 75/200 ...\n",
            "Generating image for step 76/200 ...\n",
            "Generating image for step 77/200 ...\n",
            "Generating image for step 78/200 ...\n",
            "Generating image for step 79/200 ...\n",
            "Generating image for step 80/200 ...\n",
            "Generating image for step 81/200 ...\n",
            "Generating image for step 82/200 ...\n",
            "Generating image for step 83/200 ...\n",
            "Generating image for step 84/200 ...\n",
            "Generating image for step 85/200 ...\n",
            "Generating image for step 86/200 ...\n",
            "Generating image for step 87/200 ...\n",
            "Generating image for step 88/200 ...\n",
            "Generating image for step 89/200 ...\n",
            "Generating image for step 90/200 ...\n",
            "Generating image for step 91/200 ...\n",
            "Generating image for step 92/200 ...\n",
            "Generating image for step 93/200 ...\n",
            "Generating image for step 94/200 ...\n",
            "Generating image for step 95/200 ...\n",
            "Generating image for step 96/200 ...\n",
            "Generating image for step 97/200 ...\n",
            "Generating image for step 98/200 ...\n",
            "Generating image for step 99/200 ...\n",
            "Generating image for step 100/200 ...\n",
            "Generating image for step 101/200 ...\n",
            "Generating image for step 102/200 ...\n",
            "Generating image for step 103/200 ...\n",
            "Generating image for step 104/200 ...\n",
            "Generating image for step 105/200 ...\n",
            "Generating image for step 106/200 ...\n",
            "Generating image for step 107/200 ...\n",
            "Generating image for step 108/200 ...\n",
            "Generating image for step 109/200 ...\n",
            "Generating image for step 110/200 ...\n",
            "Generating image for step 111/200 ...\n",
            "Generating image for step 112/200 ...\n",
            "Generating image for step 113/200 ...\n",
            "Generating image for step 114/200 ...\n",
            "Generating image for step 115/200 ...\n",
            "Generating image for step 116/200 ...\n",
            "Generating image for step 117/200 ...\n",
            "Generating image for step 118/200 ...\n",
            "Generating image for step 119/200 ...\n",
            "Generating image for step 120/200 ...\n",
            "Generating image for step 121/200 ...\n",
            "Generating image for step 122/200 ...\n",
            "Generating image for step 123/200 ...\n",
            "Generating image for step 124/200 ...\n",
            "Generating image for step 125/200 ...\n",
            "Generating image for step 126/200 ...\n",
            "Generating image for step 127/200 ...\n",
            "Generating image for step 128/200 ...\n",
            "Generating image for step 129/200 ...\n",
            "Generating image for step 130/200 ...\n",
            "Generating image for step 131/200 ...\n",
            "Generating image for step 132/200 ...\n",
            "Generating image for step 133/200 ...\n",
            "Generating image for step 134/200 ...\n",
            "Generating image for step 135/200 ...\n",
            "Generating image for step 136/200 ...\n",
            "Generating image for step 137/200 ...\n",
            "Generating image for step 138/200 ...\n",
            "Generating image for step 139/200 ...\n",
            "Generating image for step 140/200 ...\n",
            "Generating image for step 141/200 ...\n",
            "Generating image for step 142/200 ...\n",
            "Generating image for step 143/200 ...\n",
            "Generating image for step 144/200 ...\n",
            "Generating image for step 145/200 ...\n",
            "Generating image for step 146/200 ...\n",
            "Generating image for step 147/200 ...\n",
            "Generating image for step 148/200 ...\n",
            "Generating image for step 149/200 ...\n",
            "Generating image for step 150/200 ...\n",
            "Generating image for step 151/200 ...\n",
            "Generating image for step 152/200 ...\n",
            "Generating image for step 153/200 ...\n",
            "Generating image for step 154/200 ...\n",
            "Generating image for step 155/200 ...\n",
            "Generating image for step 156/200 ...\n",
            "Generating image for step 157/200 ...\n",
            "Generating image for step 158/200 ...\n",
            "Generating image for step 159/200 ...\n",
            "Generating image for step 160/200 ...\n",
            "Generating image for step 161/200 ...\n",
            "Generating image for step 162/200 ...\n",
            "Generating image for step 163/200 ...\n",
            "Generating image for step 164/200 ...\n",
            "Generating image for step 165/200 ...\n",
            "Generating image for step 166/200 ...\n",
            "Generating image for step 167/200 ...\n",
            "Generating image for step 168/200 ...\n",
            "Generating image for step 169/200 ...\n",
            "Generating image for step 170/200 ...\n",
            "Generating image for step 171/200 ...\n",
            "Generating image for step 172/200 ...\n",
            "Generating image for step 173/200 ...\n",
            "Generating image for step 174/200 ...\n",
            "Generating image for step 175/200 ...\n",
            "Generating image for step 176/200 ...\n",
            "Generating image for step 177/200 ...\n",
            "Generating image for step 178/200 ...\n",
            "Generating image for step 179/200 ...\n",
            "Generating image for step 180/200 ...\n",
            "Generating image for step 181/200 ...\n",
            "Generating image for step 182/200 ...\n",
            "Generating image for step 183/200 ...\n",
            "Generating image for step 184/200 ...\n",
            "Generating image for step 185/200 ...\n",
            "Generating image for step 186/200 ...\n",
            "Generating image for step 187/200 ...\n",
            "Generating image for step 188/200 ...\n",
            "Generating image for step 189/200 ...\n",
            "Generating image for step 190/200 ...\n",
            "Generating image for step 191/200 ...\n",
            "Generating image for step 192/200 ...\n",
            "Generating image for step 193/200 ...\n",
            "Generating image for step 194/200 ...\n",
            "Generating image for step 195/200 ...\n",
            "Generating image for step 196/200 ...\n",
            "Generating image for step 197/200 ...\n",
            "Generating image for step 198/200 ...\n",
            "Generating image for step 199/200 ...\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '/content/drive/MyDrive/StyleGAN2-ADA/out/latent_walk/frames/frame%05d.png':\n",
            "  Duration: 00:00:08.00, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 512x512, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mprofile High, level 3.0\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/drive/MyDrive/StyleGAN2-ADA/out/latent_walk/walk-z-circularloop-seed0-24fps.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 512x512, q=-1--1, 24 fps, 12288 tbn, 24 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  200 fps= 61 q=-1.0 Lsize=     117kB time=00:00:08.20 bitrate= 116.9kbits/s speed=2.52x    \n",
            "video:114kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.302390%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mframe I:1     Avg QP:17.74  size: 27175\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mframe P:133   Avg QP:18.48  size:   631\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mframe B:66    Avg QP:24.20  size:    83\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mconsecutive B-frames: 38.5% 48.0% 13.5%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mmb I  I16..4:  5.1% 90.1%  4.8%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mmb P  I16..4:  0.0%  0.0%  0.0%  P16..4: 13.2%  2.5%  3.9%  0.0%  0.0%    skip:80.4%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  9.2%  0.0%  0.0%  direct: 0.0%  skip:90.8%  L0: 1.1% L1:98.9% BI: 0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0m8x8 transform intra:89.9% inter:86.1%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mcoded y,uvDC,uvAC intra: 95.0% 95.3% 87.2% inter: 3.4% 5.1% 0.2%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mi16 v,h,dc,p:  7% 16% 16% 60%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17%  9% 27%  7%  7% 10%  4% 11%  8%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 14%  9%  9% 11% 15%  9% 13%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mi8c dc,h,v,p: 47% 17% 22% 15%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mref P L0: 79.0%  7.1% 11.6%  2.3%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mref B L0: 78.9% 21.1%\n",
            "\u001b[1;36m[libx264 @ 0x55db7ee79e00] \u001b[0mkb/s:111.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9878RlntrDp"
      },
      "source": [
        "## While you wait ...\n",
        "\n",
        "... learn more about Generative Adversarial Networks and StyleGAN2-ADA:\n",
        "\n",
        "*   [This Night Sky Does Not Exist](https://arthurfindelair.com/thisnightskydoesnotexist/): Generation of images from a model created using this Notebook on Google Colab Pro.\n",
        "*   [This **X** Does Not Exist](https://thisxdoesnotexist.com/): Collection of sites showing the power of GANs.\n",
        "*   [Karras, Tero, et al. _Analyzing and Improving the Image Quality of StyleGAN._ CVPR 2020.](https://arxiv.org/pdf/2006.06676.pdf): Paper published for the release of StyleGAN2-ADA.\n",
        "*   [Official implementation of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada)\n",
        "*   [StyleGAN v2: notes on training and latent space exploration](https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3): Interesting article from Toward Data Science"
      ]
    }
  ]
}